# FUSOU AVRO Hot/Cold Architecture Flow

## æ¦‚è¦
Cloudflare Workers ä¸Šã§å‹•ä½œã™ã‚‹ã€**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®Avroãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç®¡ç†ï¼†å–å¾—**ãƒ•ãƒ­ãƒ¼ã®è©³ç´°èª¬æ˜ã€‚

---

## 1. å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³

```mermaid
graph TB
        subgraph Ingest
            User["ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼<br/>(ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ)"]
            Upload["ğŸŒ POST /battle-data/upload<br/>(base64 Avro slices)"]
            Queue["ğŸ“¬ Cloudflare Queue<br/>(dev-kc-compaction-queue)"]
            Consumer["âš™ï¸ Queue Consumer<br/>(src/consumer.ts)"]
        end

        subgraph Storage
            D1["ğŸ—„ï¸ D1 Database<br/>(dev_kc_battle_index)"]
            R2["â˜ï¸ R2 Bucket<br/>(dev-kc-battle-data)"]
        end

        subgraph Batch
            Cron["â° Cron Archiver<br/>(src/cron.ts)"]
        end

        subgraph Read
            Read["ğŸŒ GET /read<br/>(?dataset_id=...&table_name=...&period_tag=...)"]
            Reader["ğŸ“– Reader<br/>(src/reader.ts)"]
            Avro["ğŸ“„ Avro OCF<br/>(ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°)"]
        end

        User -->|1. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰| Upload
        Upload -->|2. enqueue| Queue
        Queue -->|3. consume| Consumer
        Consumer -->|4. INSERT<br/>buffer_logs| D1

        Cron -->|5. SELECT<br/>buffer_logs å…¨ä»¶| D1
        Cron -->|6. 1æ™‚é–“ã«1å›ã®ã¿ PUT| R2
        Cron -->|7. UPSERT<br/>block_indexes / archived_files| D1
        Cron -->|8. DELETE<br/>buffer_logs| D1

        User -->|9. å–å¾—ãƒªã‚¯ã‚¨ã‚¹ãƒˆ| Read
        Read -->|10. lookup| D1
        D1 -->|11. block_indexes| Reader
        Reader -->|12. Range GET| R2
        Reader -->|13. ãƒ˜ãƒƒãƒ€ãƒ¼+ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ | Avro
        Avro -->|14. Avro OCF| User
```

---

## 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ãƒ­ãƒ¼ï¼ˆè©³ç´°ï¼D1ãƒãƒƒãƒ•ã‚¡ï¼‰

```mermaid
sequenceDiagram
    actor User as ãƒ¦ãƒ¼ã‚¶ãƒ¼<br/>(ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ)
    participant HTTP as Worker<br/>/battle-data/upload
    participant Queue as Queue<br/>Binding
    participant Consumer as Queue Consumer<br/>(src/consumer.ts)
    participant R2 as R2 Bucket
    participant D1 as D1 Database
    
    User->>HTTP: POST /battle-data/upload<br/>{ dataset_id, table, period_tag,<br/>slices: [base64_avro_1, ...] }
    activate HTTP
    HTTP->>HTTP: Validate payload<br/>(dataset_id, table, slices)
    HTTP->>Queue: sendBatch([<br/>{ body: { dataset_id, table,<br/>period_tag, avro_base64 } },<br/>...<br/>])
    HTTP-->>User: 202 Accepted<br/>{ status: 'accepted',<br/>enqueued: N }
    deactivate HTTP

    Queue->>Consumer: Pull batch<br/>(max_batch_size=10,<br/>timeout=30s)
    activate Consumer
    Consumer->>Consumer: Group slices by dataset/table (in-memory)
    Consumer->>D1: BULK INSERT<br/>buffer_logs(values...)
    D1-->>Consumer: Success
    
    Consumer->>Consumer: ACK all messages
    deactivate Consumer
```

**Key Points:**
- `slices` ã¯base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã€‚Consumerã¯å¾©å·â†’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³â†’ãã®ã¾ã¾D1ã«ä¿å­˜
- Queue Consumer ã¯åŒä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ãƒ†ãƒ¼ãƒ–ãƒ«ã§ãƒãƒƒãƒåŒ–ã—ã€`buffer_logs` ã«ä¸€æ‹¬INSERT
- ç¦æ­¢: ã“ã®æ®µéšã§R2ã¸ã¯ä¸€åˆ‡ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼‰
- ä»¥é™ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†ã¯Cronã«å®Œå…¨ç§»è­²

---

## 3. ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ã‚³ãƒ³ãƒ‘ã‚¯ã‚·ãƒ§ãƒ³ãƒ•ãƒ­ãƒ¼ï¼ˆCron / ä½é »åº¦ï¼‰

```mermaid
sequenceDiagram
    participant Cron as Cron Archiver<br/>(src/cron.ts)
    participant D1 as D1 Database
    participant R2 as R2 Bucket

    Cron->>Cron: 1æ™‚é–“ã«1å›ãƒˆãƒªã‚¬ãƒ¼ï¼ˆã¾ãŸã¯é–¾å€¤ï¼‰
    activate Cron
    Cron->>D1: SELECT * FROM buffer_logs<br/>(å¯¾è±¡æœŸé–“/ä¸Šé™ä»¶æ•°)
    D1-->>Cron: rows

    Cron->>Cron: Avro OCF ã‚’æ§‹ç¯‰<br/>- ãƒ˜ãƒƒãƒ€ãƒ¼ç”Ÿæˆï¼ˆschema/codecï¼‰<br/>- ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ–ãƒ­ãƒƒã‚¯åŒ–<br/>- sync marker å…±æœ‰<br/>- ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ/é•·ã•ã‚’è¨˜éŒ²

    Cron->>R2: PUT /{table_name}/{period_tag}.avro<br/>(1æ™‚é–“ã«1å›ã®ã¿ã€å…¨dataset_idã‚’å«ã‚€)
    R2-->>Cron: etag, size

    Cron->>D1: UPSERT archived_files<br/>(file_path, size, created_at, ...)
    Cron->>D1: UPSERT block_indexes<br/>(file_id, start_byte, length,<br/>dataset_id, table_name, ...)
    D1-->>Cron: Success

    Cron->>D1: DELETE FROM buffer_logs<br/>(ã‚³ãƒŸãƒƒãƒˆæ¸ˆã¿ç¯„å›²)
    D1-->>Cron: Success
    deactivate Cron
```

**Key Points:**
- R2 ã¸ã®æ›¸ãè¾¼ã¿ã¯ Cron ã®ã¿ã€1æ™‚é–“ã«1å›ï¼ˆã¾ãŸã¯é–¾å€¤ï¼‰
- Avro OCFã¯`deflate`åœ§ç¸®ï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼‰ã€‚ãƒ˜ãƒƒãƒ€ã®`avro.codec`ã¯`deflate`ã€‚
- `block_indexes` ã¯ Reader ã®ã‚¢ãƒ‰ãƒ¬ã‚¹å¸³ï¼šR2 ã® Range å–å¾—ã«ä½¿ç”¨ï¼ˆ`start_byte`ã¯ãƒ˜ãƒƒãƒ€é•·ï¼‰ã€‚
- ã‚³ãƒŸãƒƒãƒˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã¯ `buffer_logs` ã‹ã‚‰å‰Šé™¤ã—ã€ãƒ›ãƒƒãƒˆé ˜åŸŸã‚’ç¸®å°

---

## 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ•ãƒ­ãƒ¼ï¼ˆHot/Cold ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰

```mermaid
sequenceDiagram
    actor User as ãƒ¦ãƒ¼ã‚¶ãƒ¼<br/>(ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ)
    participant HTTP as Worker<br/>/read
    participant D1 as D1 Database
    participant R2 as R2 Bucket
    participant Reader as Reader<br/>(src/reader.ts)
    
    User->>HTTP: GET /read?dataset_id=USER_ID&table_name=battle<br/>&from=1234567890&to=1234599999
    activate HTTP
    HTTP->>HTTP: Parse query params:<br/>dataset_id, table_name, from, to
    
    Note over HTTP,Reader: Step 1: Hot ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæœ€æ–°1æ™‚é–“ï¼‰
    HTTP->>D1: SELECT * FROM buffer_logs<br/>WHERE dataset_id=? AND table_name=?<br/>AND timestamp >= ? AND timestamp <= ?
    D1-->>HTTP: Hot rows (BLOB)
    HTTP->>HTTP: Deserialize BLOB to JSON records
    
    Note over HTTP,Reader: Step 2: Cold ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—
    HTTP->>D1: SELECT bi.*, af.file_path, af.compression_codec<br/>FROM block_indexes bi<br/>JOIN archived_files af ON bi.file_id = af.id<br/>WHERE bi.dataset_id=? AND bi.table_name=?<br/>AND bi.end_timestamp >= ? AND bi.start_timestamp <= ?
    D1-->>HTTP: Block indexes<br/>(file_path, start_byte, length, codec)
    
    Note over HTTP,Reader: Step 3: Cold ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆä¸¦åˆ— Range Requestï¼‰
    loop For each file_path group
        HTTP->>R2: GET file_path<br/>Range: [0, 4096]<br/>(Avro header)
        R2-->>HTTP: Header buffer
        
        loop For each block in file
            HTTP->>R2: GET file_path<br/>Range: [start_byte, length]<br/>(compressed data block)
            R2-->>HTTP: Block buffer (deflate)
        end
        
        HTTP->>HTTP: Decompress & parse blocks<br/>using header schema
    end
    
    Note over HTTP,Reader: Step 4: ãƒãƒ¼ã‚¸ & é‡è¤‡æ’é™¤
    HTTP->>HTTP: Merge Hot + Cold records<br/>Sort by timestamp<br/>Deduplicate by content hash
    
    HTTP-->>User: 200 OK (JSON)<br/>{ hot_count, cold_count,<br/>record_count, records: [...] }
    deactivate HTTP
    
    User->>User: Process merged records
```

**Key Points:**
- **Hot ãƒ‡ãƒ¼ã‚¿å„ªå…ˆ**: `buffer_logs` ã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‰ã®1æ™‚é–“ä»¥å†…ï¼‰
- **Cold ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å–å¾—**: `block_indexes` ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ â†’ R2 Range Request ã§è©²å½“ãƒ–ãƒ­ãƒƒã‚¯ã®ã¿å–å¾—
- **åœ§ç¸®èªè­˜**: `archived_files.compression_codec` (deflate/null) ã‚’å‚ç…§ã—ã¦è‡ªå‹•ãƒ‡ã‚³ãƒ¼ãƒ‰
- **ãƒãƒ¼ã‚¸ & é‡è¤‡æ’é™¤**: Hot + Cold ã‚’ timestamp ã§ã‚½ãƒ¼ãƒˆã€content hash ã§é‡è¤‡æ’é™¤
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥**: Cold ãƒ‡ãƒ¼ã‚¿ã¯ immutable (max-age=3600)ã€Hot ã®ã¿ã¯çŸ­æ™‚é–“ (max-age=60)
- **ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**: `from`/`to` timestamp ç¯„å›²ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹é«˜é€Ÿçµã‚Šè¾¼ã¿
- **format=ocf ã‚ªãƒ—ã‚·ãƒ§ãƒ³**: JSON ã§ã¯ãªã Avro OCF ã‚¹ãƒˆãƒªãƒ¼ãƒ  (header + blocks) ã§è¿”å´å¯èƒ½

---

## 5. ãƒ˜ãƒƒãƒ€ãƒ¼é•·ã®æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆSync Marker ãƒ™ãƒ¼ã‚¹ï¼‰

- å…¥åŠ›: R2 ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å…ˆé ­ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆåˆæœŸ 4KBï¼‰
- æ‰‹é †:
    - `Obj\x01` ãƒã‚¸ãƒƒã‚¯ç¢ºèªï¼ˆAvro OCFï¼‰
    - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®Mapã‚’Avroã®å¯å¤‰é•·ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è¦å‰‡ã§ãƒ‡ã‚³ãƒ¼ãƒ‰
    - Mapã®ç›´å¾Œã«ç¾ã‚Œã‚‹16ãƒã‚¤ãƒˆã‚’ã€Œsync markerã€ã¨ã—ã¦ç‰¹å®š
    - ãƒ˜ãƒƒãƒ€ãƒ¼é•· = sync marker çµ‚ç«¯ã¾ã§ã®ãƒã‚¤ãƒˆé•·
    - ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒè¶³ã‚Šãšã«è§£æã§ããªã‘ã‚Œã°ã€é•·ã•ã‚’2å€ã«æ‹¡å¼µã—ã¦å†è©¦è¡Œï¼ˆä¸Šé™ 64KBï¼‰
- å‡ºåŠ›: `headerLen`ï¼ˆå¾Œç¶šã®Rangeå–å¾—ã§ `[headerLen, ...]` ã¨ã—ã¦æœ¬ä½“ã®ã¿é€£çµï¼‰

---

## 6. D1 ã‚¹ã‚­ãƒ¼ãƒï¼ˆBuffer ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

### buffer_logsï¼ˆãƒ›ãƒƒãƒˆé ˜åŸŸï¼ä¸€æ™‚ä¿å­˜ï¼‰
```sql
CREATE TABLE buffer_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    dataset_id TEXT NOT NULL,
    table_name TEXT NOT NULL,
    period_tag TEXT DEFAULT 'latest',
    payload BLOB NOT NULL,               -- Avro OCF ã®ãƒ–ãƒ­ãƒƒã‚¯ï¼NDJSON ç­‰
    content_hash TEXT,                   -- ä»»æ„ï¼šé‡è¤‡æ’é™¤ç”¨
    received_at INTEGER NOT NULL         -- å—ä¿¡æ™‚åˆ»ï¼ˆmsï¼‰
);

CREATE INDEX IF NOT EXISTS idx_buffer_logs_time
    ON buffer_logs(received_at DESC);
CREATE INDEX IF NOT EXISTS idx_buffer_logs_key
    ON buffer_logs(dataset_id, table_name, period_tag);
```

### archived_filesï¼ˆR2 ã«ä¿å­˜ã—ãŸã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®å°å¸³ï¼‰
```sql
CREATE TABLE archived_files (
    file_path TEXT PRIMARY KEY,          -- ä¾‹: battle/20251223_18.avro
    created_at INTEGER NOT NULL,
    total_bytes INTEGER,
    total_records INTEGER,
    codec TEXT,                          -- ä¾‹: 'deflate'
    schema_hash TEXT                     -- ä»»æ„ï¼šã‚¹ã‚­ãƒ¼ãƒæ•´åˆæ€§ã®æ¤œæŸ»ç”¨
);
```

### block_indexesï¼ˆRange å–å¾—ç”¨ã‚¢ãƒ‰ãƒ¬ã‚¹å¸³ï¼‰
```sql
CREATE TABLE block_indexes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    file_path TEXT NOT NULL,             -- R2 ä¸Šã® Avro ãƒ•ã‚¡ã‚¤ãƒ«
    byte_offset INTEGER NOT NULL,        -- ãƒ–ãƒ­ãƒƒã‚¯å…ˆé ­ã® R2 ã‚ªãƒ•ã‚»ãƒƒãƒˆ
    byte_length INTEGER NOT NULL,        -- åœ§ç¸®ãƒ–ãƒ­ãƒƒã‚¯é•·
    record_count INTEGER,                -- ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šè¦‹ç©ã‚‚ã‚Š
    dataset_id TEXT,
    table_name TEXT,
    period_tag TEXT,
    created_at INTEGER NOT NULL,
    FOREIGN KEY(file_path) REFERENCES archived_files(file_path)
);

CREATE INDEX IF NOT EXISTS idx_block_indexes_lookup
    ON block_indexes(dataset_id, table_name, period_tag, created_at);
```

---

## 7. ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä¾‹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®1æ—¥ã®ãƒ•ãƒ­ãƒ¼

```mermaid
timeline
    title ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®1æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ä¾‹
    
        08:00 : User uploads Avro slice 1 (5MB)
            : POST /battle-data/upload â†’ Queue â†’ Consumer
            : D1: buffer_logs ã« INSERTï¼ˆ5MBï¼‰

        10:00 : User uploads Avro slice 2 (8MB)
            : D1: buffer_logs ã«è¿½è¨˜ï¼ˆåˆè¨ˆ 13MBï¼‰

        14:00 : User uploads Avro slice 3 (520MB)
            : D1: buffer_logs ã«è¿½è¨˜ï¼ˆåˆè¨ˆ 533MBï¼‰

        18:00 : Cron èµ·å‹•ï¼ˆ1æ™‚é–“ã«1å›ï¼‰
            : D1: buffer_logs å…¨ä»¶ SELECT
            : Avro OCF ã‚’æ§‹ç¯‰ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ + ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
            : R2: PUT battle/20251223_18.avroï¼ˆClass A 1 å›ï¼‰
            : D1: block_indexes / archived_files ã‚’æ›´æ–°
            : D1: buffer_logs ã‚’ DELETEï¼ˆã‚³ãƒŸãƒƒãƒˆæ¸ˆã¿ç¯„å›²ï¼‰

        18:05 : User requests: GET /read?dataset_id=...&table_name=battle
            : Reader Step 1: D1 buffer_logs ã‹ã‚‰ Hot ãƒ‡ãƒ¼ã‚¿å–å¾—
            : Reader Step 2: D1 block_indexes ã‹ã‚‰ Cold ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢
            : Reader Step 3: R2 Range GET ã§ Cold ãƒ–ãƒ­ãƒƒã‚¯å–å¾—ï¼ˆä¸¦åˆ—ï¼‰
            : Reader Step 4: Hot + Cold ã‚’ãƒãƒ¼ã‚¸ & é‡è¤‡æ’é™¤
            : JSON ã§ãƒãƒ¼ã‚¸çµæœã‚’è¿”å´
```

---

## 8. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒªã‚«ãƒãƒª

```mermaid
graph TD
    A["Queue Consumer<br/>Process Slice"] -->|Success| B["R2 PUT + D1 INSERT"]
    A -->|Failure| C["Message.retry()"]
    C -->|Retry 3 times| D{Success?}
    D -->|Yes| B
    D -->|No| E["DLQ Message<br/>(dead_letter_queue)"]
    B -->|D1 Error| F["Message.ack()<br/>(logged)"]
    E --> G["DLQ Handler<br/>Log Error"]
    
    B -->|Idempotent| H["Segment already exists?<br/>UPDATE vs INSERT"]
    H -->|Exists| I["Overwrite etag/size"]
    H -->|New| J["Create segment record"]
```

**Strategy:**
- Queue ã® `max_retries=3`ï¼šå¤±æ•—æ™‚ã¯æœ€å¤§3å›å†è©¦è¡Œ
- DLQï¼ˆ`dev-kc-compaction-dlq`ï¼‰ï¼šæœ€çµ‚å¤±æ•—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ãƒ­ã‚°ã—ã¦ä¿å­˜
- D1 æŒ¿å…¥å¤±æ•—æ™‚ã‚‚ã€R2 PUT ã¯æˆåŠŸã—ã¦ã„ã‚‹å¯èƒ½æ€§ â†’ Segment é‡è¤‡ç™»éŒ²ã®é˜²æ­¢ã« `ON CONFLICT UPDATE` ç­‰ã‚’æ¤œè¨å¯èƒ½

---

## 9. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»åˆ¶ç´„

| é …ç›® | å€¤ | èª¬æ˜ |
|------|-----|------|
| Queue batch size | 10 | max_batch_size |
| Queue timeout | 30s | max_batch_timeout |
| R2 PUT frequency | 1/h | Cron ã®ã¿ï¼ˆClass A ã‚’æœ€å°åŒ–ï¼‰ |
| Max segment size | - | ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã¯Cronå´ã®å†…éƒ¨å®Ÿè£…ï¼ˆä»»æ„ï¼‰ |
| Max header parse | 64KB | Avro headeræ¢ç´¢ä¸Šé™ |
| R2 Range request | Range: [offset, length] | ãƒã‚¤ãƒˆå˜ä½ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæœ¬ä½“å–å¾— |
| Cache TTL (read) | 300s | public, max-age=300 |
| D1 Indexes | dataset_id, table, period_tag | é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ |

---

## 10. å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œè¡¨

| ãƒ•ãƒ­ãƒ¼æ®µéš | å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ« | é–¢é€£é–¢æ•° |
|-----------|----------|--------|
| 1. Upload endpoint | src/index.ts | POST /battle-data/upload |
| 2. Queue consumer | src/consumer.ts | ãƒãƒ«ã‚¯ INSERT to buffer_logs |
| 3. Archiver (Cron) | src/cron.ts | D1â†’Avroæ§‹ç¯‰â†’R2 PUTâ†’D1æ›´æ–° |
| 4. Read endpoint | src/index.ts | GET /read (Hot/Cold ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰) |
| 5. Reader core | src/reader.ts | fetchHotData() + fetchColdIndexes() + fetchColdData() + merge |
| 6. Header/Block parse | src/avro-manual.ts | getAvroHeaderLengthFromPrefix(), parseDeflateAvroBlock() |
| 7. D1 schema | docs/sql/d1/ | buffer_logs, block_indexes, archived_files |
| 8. Avro utilities | src/utils/avro.ts | generateHeader(), generateBlock(), generateSyncMarker() |

---

## 11. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] `wrangler.toml` ã« Queue ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºèªï¼ˆ`dev-kc-compaction-queue`ï¼‰
- [ ] D1 ãƒªãƒ¢ãƒ¼ãƒˆã¸ schema é©ç”¨ï¼š`npm run schema:remote`
- [ ] R2 ãƒã‚±ãƒƒãƒˆ permissions ç¢ºèªï¼ˆèª­ã¿æ›¸ãå¯ï¼‰
- [ ] ç’°å¢ƒå¤‰æ•° `COMPACTION_QUEUE` binding ç¢ºèª
- [ ] ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆé€šéï¼š`node test/test-hot-cold.mjs`
- [ ] ãƒ‡ãƒ—ãƒ­ã‚¤å‰ãƒ“ãƒ«ãƒ‰ï¼š`npx tsc --outDir dist && npm run deploy`
- [ ] ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œï¼šQueue tail ç›£è¦– `npx wrangler tail`
- [ ] R2 test object ç¢ºèªï¼š`npx wrangler r2 object list dev-kc-battle-data --limit 10`
- [ ] ãƒ†ã‚¹ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼š`curl -X POST https://worker.dev/battle-data/upload ...`
- [ ] ãƒ†ã‚¹ãƒˆå–å¾—ï¼š`curl https://worker.dev/read?table=battle&dataset_id=test`

---

## 12. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è²¬å‹™

- src/consumer.ts: ãƒãƒƒãƒ•ã‚¡æ‹…å½“
    - å½¹å‰²: ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã®ãƒãƒƒãƒå—ä¿¡ã€ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰æ¤œè¨¼ã€D1 `buffer_logs` ã¸ã®ä¸€æ‹¬ INSERT ã®ã¿
    - ç¦æ­¢: R2 ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã€`block_indexes`/`archived_files` æ›´æ–°ã€å‰Šé™¤ç³»æ“ä½œ
    - åŠ¹æœ: é«˜é »åº¦ã§ã‚‚ä½ã‚³ã‚¹ãƒˆï¼ˆD1 æ›¸ãè¾¼ã¿ã®ã¿ï¼‰ã€‚ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€å¤§åŒ–

- src/cron.ts: ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–/ã‚³ãƒ³ãƒ‘ã‚¯ã‚·ãƒ§ãƒ³æ‹…å½“ï¼ˆCronï¼‰
    - å½¹å‰²: å®šæœŸçš„ã« `buffer_logs` ã‚’èª­ã¿å‡ºã—ã€Avro OCF ã‚’æ§‹ç¯‰ã—ã¦ R2 ã« PUTï¼ˆ1/hï¼‰
    - ä»˜éš: `archived_files`/`block_indexes` ã‚’ UPSERTã€ã‚³ãƒŸãƒƒãƒˆæ¸ˆã¿ `buffer_logs` ã‚’ DELETE
    - ãƒãƒªã‚·ãƒ¼: ã‚¹ã‚­ãƒ¼ãƒæ•´åˆæ€§ã€sync marker å…±æœ‰ã€åœ§ç¸®ï¼ˆdeflateï¼‰ç­‰

- src/reader.ts: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­å–æ‹…å½“
    - å½¹å‰²: Hot = D1 `buffer_logs`ã€Cold = D1 `block_indexes` â†’ R2 Range å–å¾—
    - ä»˜éš: Avro ãƒ˜ãƒƒãƒ€ãƒ¼é•·ã‚’ sync marker ã§ç‰¹å®šã€ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã€ãƒãƒ¼ã‚¸

- src/index.ts: HTTPã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
    - å½¹å‰²: `/battle-data/upload` ã¨ `/read` ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°/å…¥å‡ºåŠ›

--

## ã¾ã¨ã‚

1. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰** â†’ `/battle-data/upload`ï¼ˆbase64 slicesï¼‰ â†’ Queue
2. **Queue Consumer** â†’ D1 `buffer_logs` ã¸ä¸€æ‹¬ INSERTï¼ˆR2 æ›¸ãè¾¼ã¿ã‚¼ãƒ­ï¼‰
3. **Cron Archiver** â†’ 1æ™‚é–“ã«1å›ã€Avroæ§‹ç¯‰â†’R2 PUTâ†’`block_indexes`æ›´æ–°â†’`buffer_logs`å‰Šé™¤
4. **ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—** â†’ `/read` ã§ D1 `buffer_logs` + `block_indexes` ã‚’å‚ç…§ã€R2 ã¯ Range GET ã®ã¿
5. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£/ã‚³ã‚¹ãƒˆ** â†’ R2 Class A ã‚’ Cron ã®ã¿ã¸é›†ç´„ã€Range ã§åŠ¹ç‡èª­å–ã€D1 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿
