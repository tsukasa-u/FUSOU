# Hot/Cold Architecture - ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰

## âœ… ãƒ†ã‚¹ãƒˆã®æº–å‚™å®Œäº†

### 1. ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿
```bash
âœ“ TypeScript â†’ JavaScript (dist/)
âœ“ ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½
âœ“ Avroç”Ÿæˆå‹•ä½œç¢ºèªæ¸ˆã¿
```

### 2. D1ã‚¹ã‚­ãƒ¼ãƒé©ç”¨æ¸ˆã¿
```bash
âœ“ buffer_logs (Hot Storage)
âœ“ archived_files (Cold File Registry)
âœ“ block_indexes (Range Request Index)
```

## ğŸš€ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆæ‰‹é †

### Step 1: Wrangler Devèµ·å‹•
```bash
cd /home/ogu-h/Documents/GitHub/FUSOU/packages/FUSOU-WORKFLOW
npx wrangler dev --local
```

### Step 2: Buffer Consumer ãƒ†ã‚¹ãƒˆ (Hot Storage)

**POST ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡:**
```bash
curl -X POST http://localhost:8787/v1/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "dataset_id": "test-user-001",
    "table": "battle",
    "records": [
      {"timestamp": 1703302800000, "api_no": 1, "result": "S", "data": "test1"},
      {"timestamp": 1703302860000, "api_no": 2, "result": "A", "data": "test2"},
      {"timestamp": 1703302920000, "api_no": 3, "result": "B", "data": "test3"}
    ],
    "uploaded_by": "local-test"
  }'
```

**D1ã§ç¢ºèª:**
```bash
npx wrangler d1 execute dev_kc_battle_index --local \
  --command="SELECT COUNT(*) as count, dataset_id, table_name FROM buffer_logs GROUP BY dataset_id, table_name"
```

### Step 3: Archiver ãƒ†ã‚¹ãƒˆ (Hot â†’ Cold)

**æ‰‹å‹•ãƒˆãƒªã‚¬ãƒ¼:**
```bash
curl http://localhost:8787/__scheduled?cron=*
```

ã¾ãŸã¯ã€Archiverã‚’ç›´æ¥å‘¼ã³å‡ºã™ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ:
```typescript
// cron.ts ã«è¿½åŠ 
export default {
  async fetch(request: Request, env: Env) {
    if (request.url.endsWith('/archive')) {
      await handleArchiver(env);
      return new Response('Archival complete', { status: 200 });
    }
    return new Response('Not found', { status: 404 });
  },
  scheduled: handleArchiver
};
```

**ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç¢ºèª:**
```bash
# archived_files ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª
npx wrangler d1 execute dev_kc_battle_index --local \
  --command="SELECT id, file_path, file_size, compression_codec FROM archived_files"

# block_indexes ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª
npx wrangler d1 execute dev_kc_battle_index --local \
  --command="SELECT dataset_id, table_name, file_id, record_count, start_byte, length FROM block_indexes"

# buffer_logs ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç¢ºèª
npx wrangler d1 execute dev_kc_battle_index --local \
  --command="SELECT COUNT(*) as remaining FROM buffer_logs"
```

### Step 4: Reader ãƒ†ã‚¹ãƒˆ (Hot + Cold Merge)

**æ–°ã—ã„Hotãƒ‡ãƒ¼ã‚¿è¿½åŠ :**
```bash
curl -X POST http://localhost:8787/v1/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "dataset_id": "test-user-001",
    "table": "battle",
    "records": [
      {"timestamp": 1703303000000, "api_no": 4, "result": "S", "data": "test4"}
    ],
    "uploaded_by": "local-test"
  }'
```

**Hot + Cold èª­ã¿å–ã‚Š:**
```bash
curl "http://localhost:8787/v1/read?dataset_id=test-user-001&table_name=battle"
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ:**
```json
{
  "records": [
    {"timestamp": 1703302800000, "api_no": 1, "result": "S", "data": "test1"},
    {"timestamp": 1703302860000, "api_no": 2, "result": "A", "data": "test2"},
    {"timestamp": 1703302920000, "api_no": 3, "result": "B", "data": "test3"},
    {"timestamp": 1703303000000, "api_no": 4, "result": "S", "data": "test4"}
  ],
  "record_count": 4,
  "hot_count": 1,
  "cold_count": 3
}
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç¢ºèª

### å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
```
1. POST /v1/ingest â†’ buffer_logs (Hot)
2. Scheduled Worker â†’ Hot â†’ Cold
   - R2: battle/YYYYMMDD_HH.avro
   - D1: archived_files + block_indexes
   - D1: DELETE FROM buffer_logs
3. GET /v1/read â†’ merge(Hot, Cold)
```

### è¨ºæ–­ã‚¯ã‚¨ãƒª

**Hot/Cold ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:**
```sql
SELECT * FROM hot_cold_summary;
```

**ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–åŠ¹ç‡:**
```sql
SELECT * FROM archive_efficiency;
```

**ç‰¹å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ–ãƒ­ãƒƒã‚¯:**
```sql
SELECT 
  bi.id, bi.table_name, bi.record_count,
  bi.start_byte, bi.length,
  af.file_path, af.compression_codec
FROM block_indexes bi
JOIN archived_files af ON bi.file_id = af.id
WHERE bi.dataset_id = 'test-user-001'
ORDER BY bi.start_timestamp;
```

## ğŸ§ª FUSOU-DATABASEã®Avroãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸãƒ†ã‚¹ãƒˆ

### Avroãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰

```bash
# FUSOU-DATABASEã®ãƒ‘ã‚¹ç¢ºèª
ls -la /home/ogu-h/Documents/GitHub/FUSOU/packages/FUSOU-DATABASE/fusou/2025-11-05/master_data/
```

åˆ©ç”¨å¯èƒ½ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:
- `mst_ships.avro` - è‰¦èˆ¹ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
- `mst_slot_items.avro` - è£…å‚™ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
- `mst_map_infos.avro` - æµ·åŸŸãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿

### Node.jsã§Avroã‚’JSONã«å¤‰æ›ã—ã¦POST

```javascript
// test/load-fusou-data.mjs
import { readFileSync } from 'fs';
import avroLib from 'avro-js';
const avro = avroLib;

const avroFile = '/home/ogu-h/Documents/GitHub/FUSOU/packages/FUSOU-DATABASE/fusou/2025-11-05/master_data/mst_ships.avro';
const data = readFileSync(avroFile);
const decoder = avro.createFileDecoder(data);

const records = [];
for (const record of decoder) {
  records.push(record);
}

// POST to buffer consumer
await fetch('http://localhost:8787/v1/ingest', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    dataset_id: 'fusou-master',
    table: 'ships',
    records: records.slice(0, 100), // æœ€åˆã®100ä»¶
    uploaded_by: 'test-loader'
  })
});
```

## ğŸ¯ ãƒ†ã‚¹ãƒˆæˆåŠŸåŸºæº–

- [ ] Buffer Consumer: POSTã§`buffer_logs`ã«æŒ¿å…¥ã•ã‚Œã‚‹
- [ ] Archiver: `buffer_logs` â†’ R2 + `block_indexes` â†’ `buffer_logs`ãŒç©º
- [ ] Reader: Hot + Coldã‚’ãƒãƒ¼ã‚¸ã—ã¦æ­£ã—ã„ä»¶æ•°ã‚’è¿”ã™
- [ ] åœ§ç¸®: R2ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒdeflateåœ§ç¸®ã•ã‚Œã¦ã„ã‚‹
- [ ] Range Request: è¤‡æ•°ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã€ä¸¦åˆ—å–å¾—ã•ã‚Œã‚‹
- [ ] Deduplication: åŒã˜`content-hash`ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒé‡è¤‡ã—ãªã„

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼
```bash
# TypeScriptå†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
npx tsc --outDir dist
```

### D1ãƒ†ãƒ¼ãƒ–ãƒ«æœªä½œæˆ
```bash
# ã‚¹ã‚­ãƒ¼ãƒå†é©ç”¨
npx wrangler d1 execute dev_kc_battle_index --local \
  --file=../../docs/sql/d1/hot-cold-schema.sql
```

### R2ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæœªä½œæˆ
- ArchiverãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
- `archived_files`ãƒ†ãƒ¼ãƒ–ãƒ«ã«`file_path`ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

### Range Requestã‚¨ãƒ©ãƒ¼
- `block_indexes`ã®`start_byte`ã¨`length`ãŒæ­£ã—ã„ã‹ç¢ºèª
- R2ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨åˆè‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèª

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **wrangler.tomlã®æ›´æ–°**
   - Hot/Coldç”¨ã®Queue bindingè¿½åŠ 
   - Scheduled Workerè¨­å®š (Archiver)

2. **æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™**
   - ãƒªãƒ¢ãƒ¼ãƒˆD1ã‚¹ã‚­ãƒ¼ãƒé©ç”¨
   - R2ãƒã‚±ãƒƒãƒˆä½œæˆ
   - ç’°å¢ƒå¤‰æ•°è¨­å®š

3. **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®š**
   - Cloudflare Analytics
   - D1 query metrics
   - R2 storage metrics
